# cython: language_level=3
# cython: boundscheck=False
# cython: wraparound=False
# cython: cdivision=True

"""
Advanced Malware Detection Engine
Ensemble methods, deep behavioral analysis, and sophisticated heuristics
"""

import numpy as np
cimport numpy as cnp
from libc.math cimport log, exp, sqrt, fabs, tanh, pow
from libc.stdlib cimport malloc, free
from libc.string cimport memcpy
import cython

cnp.import_array()

ctypedef cnp.float64_t DTYPE_t
ctypedef cnp.uint8_t BYTE_t

@cython.boundscheck(False)
@cython.wraparound(False)
cdef class DeepBehavioralAnalyzer:
    """Advanced behavioral pattern analysis"""

    cdef double[:] behavior_patterns
    cdef double[:, :] temporal_patterns
    cdef int pattern_count
    cdef double[:] anomaly_scores
    cdef double[:, :] pattern_transitions

    def __init__(self, int pattern_count=100):
        self.pattern_count = pattern_count
        self.behavior_patterns = np.zeros(pattern_count, dtype=np.float64)
        self.temporal_patterns = np.zeros((pattern_count, pattern_count), dtype=np.float64)
        self.anomaly_scores = np.zeros(pattern_count, dtype=np.float64)
        self.pattern_transitions = np.zeros((pattern_count, pattern_count), dtype=np.float64)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double analyze_api_call_sequence(self, unsigned char* data, int length) nogil:
        """Analyze API call patterns for suspicious behavior"""
        cdef int i, j
        cdef int suspicious_calls = 0
        cdef int total_calls = 0
        cdef double suspicion_score = 0.0

        # Suspicious API patterns (simplified for static analysis)
        cdef unsigned char suspicious_patterns[10][4]
        suspicious_patterns[0][:] = [0x FF, 0x15, 0x00, 0x00]  # Call [mem]
        suspicious_patterns[1][:] = [0x E8, 0x00, 0x00, 0x00]  # Call relative
        suspicious_patterns[2][:] = [0x0F, 0x34, 0x00, 0x00]  # SYSENTER
        suspicious_patterns[3][:] = [0x0F, 0x05, 0x00, 0x00]  # SYSCALL

        for i in range(length - 4):
            for j in range(4):
                if (data[i] == suspicious_patterns[j][0] and
                    data[i+1] == suspicious_patterns[j][1]):
                    suspicious_calls += 1
                    total_calls += 1

        if total_calls > 0:
            suspicion_score = <double>suspicious_calls / <double>total_calls

        return suspicion_score

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double detect_injection_patterns(self, unsigned char* data, int length) nogil:
        """Detect code injection patterns"""
        cdef int i
        cdef double injection_score = 0.0
        cdef int shellcode_indicators = 0

        # Common shellcode patterns
        for i in range(length - 8):
            # NOP sled detection
            if (data[i] == 0x90 and data[i+1] == 0x90 and
                data[i+2] == 0x90 and data[i+3] == 0x90):
                injection_score += 0.1

            # GetPC code (common in shellcode)
            if data[i] == 0xE8 and data[i+1] == 0x00:  # CALL $+0
                shellcode_indicators += 1

            # Stack pivot operations
            if data[i] == 0x87 or data[i] == 0x94:  # XCHG ESP
                shellcode_indicators += 1

        injection_score += <double>shellcode_indicators / 100.0
        return tanh(injection_score)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double analyze_memory_patterns(self, unsigned char* data, int length) nogil:
        """Analyze memory access patterns"""
        cdef int i
        cdef double pattern_score = 0.0
        cdef int heap_spray_indicators = 0
        cdef int repetitive_blocks = 0
        cdef unsigned char prev_byte = 0

        # Detect heap spray patterns (repetitive data)
        for i in range(length):
            if i > 0 and data[i] == prev_byte:
                repetitive_blocks += 1

            prev_byte = data[i]

            # Common heap spray bytes
            if data[i] == 0x0C or data[i] == 0x90 or data[i] == 0xCC:
                heap_spray_indicators += 1

        pattern_score = (<double>heap_spray_indicators / <double>length +
                        <double>repetitive_blocks / <double>length)

        return tanh(pattern_score * 10.0)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double detect_anti_analysis(self, unsigned char* data, int length) nogil:
        """Detect anti-analysis and evasion techniques"""
        cdef int i
        cdef double evasion_score = 0.0
        cdef int evasion_techniques = 0

        for i in range(length - 4):
            # RDTSC (timing checks)
            if data[i] == 0x0F and data[i+1] == 0x31:
                evasion_techniques += 2

            # CPUID
            if data[i] == 0x0F and data[i+1] == 0xA2:
                evasion_techniques += 2

            # IsDebuggerPresent checks
            if data[i] == 0x64 and data[i+1] == 0xA1:  # MOV EAX, FS:[...]
                evasion_techniques += 1

            # INT 3 (software breakpoint)
            if data[i] == 0xCC:
                evasion_techniques += 1

            # INT 2D (anti-debug)
            if data[i] == 0xCD and data[i+1] == 0x2D:
                evasion_techniques += 3

        evasion_score = tanh(<double>evasion_techniques / 50.0)
        return evasion_score

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cpdef dict analyze(self, bytes file_data):
        """Complete behavioral analysis"""
        cdef unsigned char* data = <unsigned char*><char*>file_data
        cdef int length = len(file_data)

        cdef double api_score = self.analyze_api_call_sequence(data, length)
        cdef double injection_score = self.detect_injection_patterns(data, length)
        cdef double memory_score = self.analyze_memory_patterns(data, length)
        cdef double evasion_score = self.detect_anti_analysis(data, length)

        # Combined behavioral score
        cdef double behavioral_score = (api_score * 0.3 +
                                        injection_score * 0.3 +
                                        memory_score * 0.2 +
                                        evasion_score * 0.2)

        return {
            'behavioral_score': float(behavioral_score),
            'api_suspicion': float(api_score),
            'injection_indicators': float(injection_score),
            'memory_anomalies': float(memory_score),
            'evasion_techniques': float(evasion_score),
            'is_suspicious': behavioral_score > 0.5
        }

@cython.boundscheck(False)
@cython.wraparound(False)
cdef class AdvancedHeuristicEngine:
    """Sophisticated heuristic detection algorithms"""

    cdef double[:] heuristic_weights
    cdef int n_heuristics
    cdef double[:] feature_importance

    def __init__(self, int n_heuristics=50):
        self.n_heuristics = n_heuristics
        self.heuristic_weights = np.random.uniform(0.5, 1.5, n_heuristics).astype(np.float64)
        self.feature_importance = np.zeros(n_heuristics, dtype=np.float64)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double check_packer_signatures(self, unsigned char* data, int length) nogil:
        """Detect packer signatures"""
        cdef int i
        cdef int packer_indicators = 0

        # UPX signature
        if length > 4:
            if data[0] == 0x55 and data[1] == 0x50 and data[2] == 0x58:
                packer_indicators += 3

        # High entropy sections (packed data)
        cdef int section_size = 256
        cdef int n_sections = length // section_size
        cdef double entropy

        for i in range(n_sections):
            entropy = self.calculate_local_entropy(&data[i * section_size], section_size)
            if entropy > 7.5:  # Very high entropy
                packer_indicators += 1

        return tanh(<double>packer_indicators / 10.0)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double calculate_local_entropy(self, unsigned char* data, int length) nogil:
        """Calculate Shannon entropy for a data block"""
        cdef unsigned int counts[256]
        cdef int i
        cdef double entropy = 0.0
        cdef double prob

        for i in range(256):
            counts[i] = 0

        for i in range(length):
            counts[data[i]] += 1

        for i in range(256):
            if counts[i] > 0:
                prob = <double>counts[i] / <double>length
                entropy -= prob * log(prob) / log(2.0)

        return entropy

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double detect_obfuscation(self, unsigned char* data, int length) nogil:
        """Detect code obfuscation"""
        cdef int i
        cdef double obfuscation_score = 0.0
        cdef int junk_instructions = 0
        cdef int complex_instructions = 0

        for i in range(length - 4):
            # Junk instructions (redundant operations)
            if data[i] == 0x90:  # NOP
                junk_instructions += 1
            elif data[i] == 0x40 or data[i] == 0x48:  # INC/DEC
                if i + 1 < length and (data[i+1] == 0x48 or data[i+1] == 0x40):
                    junk_instructions += 1  # Opposite operations

            # Unusual instruction sequences
            if data[i] == 0x0F:  # Multi-byte instructions
                complex_instructions += 1

        obfuscation_score = ((<double>junk_instructions / <double>length) +
                            (<double>complex_instructions / <double>length) * 2.0)

        return tanh(obfuscation_score * 10.0)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double analyze_string_artifacts(self, unsigned char* data, int length) nogil:
        """Analyze strings for suspicious content"""
        cdef int i, j
        cdef int suspicious_strings = 0
        cdef int printable_chars = 0
        cdef double string_score = 0.0

        # Common malware-related strings (simplified check)
        for i in range(length - 8):
            # Check for printable ASCII sequences (potential strings)
            if (data[i] >= 0x20 and data[i] <= 0x7E and
                data[i+1] >= 0x20 and data[i+1] <= 0x7E and
                data[i+2] >= 0x20 and data[i+2] <= 0x7E):
                printable_chars += 1

                # Check for suspicious patterns in strings
                # "cmd", "exe", "dll", "bat", "ps1"
                if ((data[i] == ord('c') or data[i] == ord('C')) and
                    (data[i+1] == ord('m') or data[i+1] == ord('M')) and
                    (data[i+2] == ord('d') or data[i+2] == ord('D'))):
                    suspicious_strings += 1

        # Very few strings might indicate obfuscation/encryption
        cdef double string_ratio = <double>printable_chars / <double>length
        if string_ratio < 0.01 or string_ratio > 0.5:
            string_score += 0.3

        string_score += <double>suspicious_strings / 100.0
        return tanh(string_score)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double detect_privilege_escalation(self, unsigned char* data, int length) nogil:
        """Detect privilege escalation indicators"""
        cdef int i
        cdef int escalation_indicators = 0

        for i in range(length - 4):
            # Token manipulation
            if data[i] == 0x68:  # PUSH
                escalation_indicators += 1

            # Process creation with elevated privileges
            if data[i] == 0xFF and data[i+1] == 0x15:  # CALL [mem]
                escalation_indicators += 1

        return tanh(<double>escalation_indicators / 100.0)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cpdef double[:] extract_heuristic_features(self, bytes file_data):
        """Extract all heuristic features"""
        cdef unsigned char* data = <unsigned char*><char*>file_data
        cdef int length = len(file_data)
        cdef double[:] features = np.zeros(self.n_heuristics, dtype=np.float64)

        # Extract features
        features[0] = self.check_packer_signatures(data, length)
        features[1] = self.detect_obfuscation(data, length)
        features[2] = self.analyze_string_artifacts(data, length)
        features[3] = self.detect_privilege_escalation(data, length)

        # Additional features
        features[4] = self.calculate_local_entropy(data, length)
        features[5] = <double>length / 1000000.0  # File size feature

        # File structure features
        cdef int sections = 0
        cdef int executable_sections = 0

        for i in range(min(length, 1000), 0, -100):
            if data[i] != 0:
                sections += 1

        features[6] = <double>sections / 10.0

        return features

@cython.boundscheck(False)
@cython.wraparound(False)
cdef class EnsembleDetector:
    """Ensemble detection using multiple models with voting"""

    cdef int n_models
    cdef double[:] model_weights
    cdef double[:] model_biases
    cdef double voting_threshold
    cdef list detection_history

    def __init__(self, int n_models=10):
        self.n_models = n_models
        self.model_weights = np.random.uniform(0.7, 1.3, n_models).astype(np.float64)
        self.model_biases = np.random.uniform(-0.1, 0.1, n_models).astype(np.float64)
        self.voting_threshold = 0.6
        self.detection_history = []

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double model_predict(self, int model_idx, double[:] features) nogil:
        """Individual model prediction"""
        cdef double prediction = self.model_biases[model_idx]
        cdef int i

        for i in range(min(len(features), 20)):
            prediction += features[i] * self.model_weights[model_idx] * (1.0 + <double>i / 100.0)

        return tanh(prediction)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef double weighted_voting(self, double[:] predictions) nogil:
        """Weighted voting across models"""
        cdef double total_weight = 0.0
        cdef double weighted_sum = 0.0
        cdef int i

        for i in range(self.n_models):
            weighted_sum += predictions[i] * self.model_weights[i]
            total_weight += self.model_weights[i]

        return weighted_sum / total_weight

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cpdef dict ensemble_predict(self, double[:] features):
        """Ensemble prediction with voting"""
        cdef double[:] predictions = np.zeros(self.n_models, dtype=np.float64)
        cdef int i
        cdef int votes_malicious = 0
        cdef int votes_benign = 0

        # Get predictions from all models
        for i in range(self.n_models):
            predictions[i] = self.model_predict(i, features)

            if predictions[i] > 0.5:
                votes_malicious += 1
            else:
                votes_benign += 1

        # Weighted voting
        cdef double ensemble_score = self.weighted_voting(predictions)

        # Confidence calculation
        cdef double confidence = fabs(ensemble_score - 0.5) * 2.0

        # Final decision
        cdef bint is_malicious = ensemble_score > self.voting_threshold

        return {
            'ensemble_score': float(ensemble_score),
            'is_malicious': is_malicious,
            'confidence': float(confidence),
            'votes_malicious': votes_malicious,
            'votes_benign': votes_benign,
            'model_agreement': float(max(votes_malicious, votes_benign)) / float(self.n_models)
        }

@cython.boundscheck(False)
@cython.wraparound(False)
cdef class DeepFeatureExtractor:
    """Deep hierarchical feature extraction"""

    cdef double[:, :] layer1_weights
    cdef double[:, :] layer2_weights
    cdef double[:, :] layer3_weights
    cdef double[:] layer1_bias
    cdef double[:] layer2_bias
    cdef double[:] layer3_bias
    cdef int input_dim, hidden1_dim, hidden2_dim, output_dim

    def __init__(self, int input_dim=256, int hidden1_dim=128,
                 int hidden2_dim=64, int output_dim=32):
        self.input_dim = input_dim
        self.hidden1_dim = hidden1_dim
        self.hidden2_dim = hidden2_dim
        self.output_dim = output_dim

        # Initialize weights with Xavier initialization
        cdef double scale1 = sqrt(2.0 / (input_dim + hidden1_dim))
        cdef double scale2 = sqrt(2.0 / (hidden1_dim + hidden2_dim))
        cdef double scale3 = sqrt(2.0 / (hidden2_dim + output_dim))

        self.layer1_weights = np.random.randn(input_dim, hidden1_dim).astype(np.float64) * scale1
        self.layer2_weights = np.random.randn(hidden1_dim, hidden2_dim).astype(np.float64) * scale2
        self.layer3_weights = np.random.randn(hidden2_dim, output_dim).astype(np.float64) * scale3

        self.layer1_bias = np.zeros(hidden1_dim, dtype=np.float64)
        self.layer2_bias = np.zeros(hidden2_dim, dtype=np.float64)
        self.layer3_bias = np.zeros(output_dim, dtype=np.float64)

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef void relu(self, double[:] x) nogil:
        """ReLU activation"""
        cdef int i
        for i in range(len(x)):
            if x[i] < 0.0:
                x[i] = 0.0

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef void batch_norm(self, double[:] x) nogil:
        """Simplified batch normalization"""
        cdef int i
        cdef double mean = 0.0, variance = 0.0
        cdef int n = len(x)

        # Calculate mean
        for i in range(n):
            mean += x[i]
        mean /= <double>n

        # Calculate variance
        for i in range(n):
            variance += (x[i] - mean) * (x[i] - mean)
        variance /= <double>n

        # Normalize
        cdef double std = sqrt(variance + 1e-8)
        for i in range(n):
            x[i] = (x[i] - mean) / std

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cpdef double[:] extract_deep_features(self, double[:] input_features):
        """Extract deep hierarchical features"""
        cdef int i, j

        # Layer 1
        cdef double[:] hidden1 = np.zeros(self.hidden1_dim, dtype=np.float64)
        for i in range(self.hidden1_dim):
            hidden1[i] = self.layer1_bias[i]
            for j in range(min(len(input_features), self.input_dim)):
                hidden1[i] += self.layer1_weights[j, i] * input_features[j]

        self.relu(hidden1)
        self.batch_norm(hidden1)

        # Layer 2
        cdef double[:] hidden2 = np.zeros(self.hidden2_dim, dtype=np.float64)
        for i in range(self.hidden2_dim):
            hidden2[i] = self.layer2_bias[i]
            for j in range(self.hidden1_dim):
                hidden2[i] += self.layer2_weights[j, i] * hidden1[j]

        self.relu(hidden2)
        self.batch_norm(hidden2)

        # Layer 3 (output)
        cdef double[:] output = np.zeros(self.output_dim, dtype=np.float64)
        for i in range(self.output_dim):
            output[i] = self.layer3_bias[i]
            for j in range(self.hidden2_dim):
                output[i] += self.layer3_weights[j, i] * hidden2[j]

        # Final activation
        for i in range(self.output_dim):
            output[i] = tanh(output[i])

        return output

@cython.boundscheck(False)
@cython.wraparound(False)
cdef class AdaptiveThresholdingSystem:
    """Adaptive thresholding based on recent detections"""

    cdef double base_threshold
    cdef double current_threshold
    cdef double[:] recent_scores
    cdef int history_size
    cdef int history_count
    cdef double adaptation_rate

    def __init__(self, double base_threshold=0.7, int history_size=100):
        self.base_threshold = base_threshold
        self.current_threshold = base_threshold
        self.history_size = history_size
        self.recent_scores = np.zeros(history_size, dtype=np.float64)
        self.history_count = 0
        self.adaptation_rate = 0.01

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef void update_history(self, double score) nogil:
        """Update detection history"""
        cdef int idx = self.history_count % self.history_size
        self.recent_scores[idx] = score
        self.history_count += 1

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cdef void adapt_threshold(self) nogil:
        """Adapt threshold based on recent scores"""
        if self.history_count < 10:
            return

        cdef int n = min(self.history_count, self.history_size)
        cdef double mean = 0.0, std = 0.0
        cdef int i

        # Calculate mean
        for i in range(n):
            mean += self.recent_scores[i]
        mean /= <double>n

        # Calculate std
        for i in range(n):
            std += (self.recent_scores[i] - mean) * (self.recent_scores[i] - mean)
        std = sqrt(std / <double>n)

        # Adapt threshold
        cdef double target = mean + std * 0.5
        self.current_threshold += (target - self.current_threshold) * self.adaptation_rate

        # Clamp threshold
        if self.current_threshold < 0.3:
            self.current_threshold = 0.3
        elif self.current_threshold > 0.9:
            self.current_threshold = 0.9

    @cython.boundscheck(False)
    @cython.wraparound(False)
    cpdef bint classify(self, double score):
        """Classify with adaptive threshold"""
        self.update_history(score)
        self.adapt_threshold()
        return score > self.current_threshold

    cpdef double get_current_threshold(self):
        """Get current adaptive threshold"""
        return self.current_threshold
